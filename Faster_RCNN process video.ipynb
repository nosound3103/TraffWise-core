{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T03:25:02.607109Z",
     "iopub.status.busy": "2025-03-03T03:25:02.606821Z",
     "iopub.status.idle": "2025-03-03T03:25:02.619982Z",
     "shell.execute_reply": "2025-03-03T03:25:02.618963Z",
     "shell.execute_reply.started": "2025-03-03T03:25:02.607086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms.functional as F\n",
    "import os\n",
    "from torchvision.ops import nms\n",
    "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
    "from torchvision.models.detection.backbone_utils import BackboneWithFPN\n",
    "from torchvision.models import mobilenet_v3_small\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models import MobileNet_V3_Small_Weights\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(640, 640)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_to_idx = {\n",
    "    \"bus\": 0, \"car\": 1, \"motorbike\": 2, \"truck\": 3,\n",
    "}\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 image_dir,\n",
    "                 class_to_idx=class_to_idx,\n",
    "                 transform=data_transform):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx if class_to_idx else {}\n",
    "\n",
    "        self.image_files = [f for f in os.listdir(\n",
    "            self.image_dir) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_filename = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        original_width, original_height = img.size\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        new_width, new_height = 640, 640\n",
    "        scale_x = new_width / original_width\n",
    "        scale_y = new_height / original_height\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRCNNTransform(GeneralizedRCNNTransform):\n",
    "    def __init__(self):\n",
    "        super().__init__(min_size=640, max_size=640, image_mean=[\n",
    "            0.485, 0.456, 0.406], image_std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def resize(self, image, target):\n",
    "        image = F.resize(image, [640, 640])\n",
    "\n",
    "        if target is not None and \"boxes\" in target:\n",
    "            w_old, h_old = image.shape[-1], image.shape[-2]\n",
    "            w_new, h_new = 640, 640\n",
    "            scale_w = w_new / w_old\n",
    "            scale_h = h_new / h_old\n",
    "            target[\"boxes\"][:, [0, 2]] *= scale_w\n",
    "            target[\"boxes\"][:, [1, 3]] *= scale_h\n",
    "        return image, target\n",
    "\n",
    "\n",
    "class FRCNN(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_classes,\n",
    "                 pretrained=MobileNet_V3_Small_Weights.DEFAULT):\n",
    "        super(FRCNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.backbone = self.get_backbone(pretrained)\n",
    "\n",
    "        self.anchor_sizes = (32, 64, 128, 256)\n",
    "        self.aspect_ratios = ((0.5, 1.0, 2.0),) * len(self.anchor_sizes)\n",
    "\n",
    "        self.anchor_generator = AnchorGenerator(\n",
    "            sizes=self.anchor_sizes,\n",
    "            aspect_ratios=self.aspect_ratios\n",
    "        )\n",
    "\n",
    "        self.model = FasterRCNN(\n",
    "            backbone=self.backbone,\n",
    "            num_classes=num_classes,\n",
    "            rpn_anchor_generator=self.anchor_generator\n",
    "        )\n",
    "\n",
    "        self.model.transform = CustomRCNNTransform()\n",
    "\n",
    "    def get_backbone(self, pretrained):\n",
    "        backbone = mobilenet_v3_small(weights=pretrained).features\n",
    "        return_layers = {'2': '0', '7': '1', '12': '2'}\n",
    "        in_channels = [24, 48, 576]\n",
    "\n",
    "        backbone.out_channels = 64\n",
    "        fpn = BackboneWithFPN(\n",
    "            backbone=backbone,\n",
    "            return_layers=return_layers,\n",
    "            in_channels_list=in_channels,\n",
    "            out_channels=64\n",
    "        )\n",
    "\n",
    "        return fpn\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        if self.training:\n",
    "            if targets is None:\n",
    "                raise ValueError(\"In training mode, targets should be passed\")\n",
    "            return self.model(images, targets)\n",
    "        else:\n",
    "            return self.model(images)\n",
    "\n",
    "\n",
    "\n",
    "# Define a custom collate function outside other functions\n",
    "def detection_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5404872\n",
      "Trainable parameters: 5404872\n"
     ]
    }
   ],
   "source": [
    "model = FRCNN(num_classes=5)  # Khởi tạo model với số lớp cần thiết\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T03:24:20.089233Z",
     "iopub.status.busy": "2025-03-03T03:24:20.088940Z",
     "iopub.status.idle": "2025-03-03T03:24:20.097718Z",
     "shell.execute_reply": "2025-03-03T03:24:20.096817Z",
     "shell.execute_reply.started": "2025-03-03T03:24:20.089208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def draw_bounding_boxes(image, boxes, labels, scores, class_names, threshold=0.7):\n",
    "    fig, ax = plt.subplots(1, figsize=(12, 12))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score >= threshold:  # Chỉ hiển thị các box có score cao hơn ngưỡng\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            width = x_max - x_min\n",
    "            height = y_max - y_min\n",
    "\n",
    "            # Vẽ hình chữ nhật\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min), width, height, linewidth=2, edgecolor='r', facecolor='none'\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Thêm tên class và score\n",
    "            class_name = class_names[label - 1]  # -1 vì label bắt đầu từ 1, class_names từ 0\n",
    "            label_text = f\"{class_name}: {score:.2f}\"\n",
    "            plt.text(\n",
    "                x_min, y_min - 10,  # Vị trí văn bản (phía trên box)\n",
    "                label_text,\n",
    "                color='white',\n",
    "                fontsize=12,\n",
    "                bbox=dict(facecolor='red', alpha=0.5)  # Hộp nền đỏ cho văn bản\n",
    "            )\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_path, video_path, class_names, nms_threshold=0.5, output_path=None, threshold=0.7):\n",
    "    # Device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model từ checkpoint\n",
    "    model = FRCNN(num_classes=len(class_names) + 1)  # +1 cho background\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Mở file video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Không thể mở file video!\")\n",
    "        return\n",
    "\n",
    "    # Lấy thông tin video\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    # Nếu có output_path, tạo video đầu ra với codec H.264\n",
    "    if output_path:\n",
    "        # Sử dụng codec H.264 (AVC). Lưu ý: Cần OpenCV build với FFmpeg để H264 hoạt động\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'H264')  # Thử với H264\n",
    "        # Nếu H264 không hoạt động, có thể thay bằng 'X264' hoặc kiểm tra build OpenCV\n",
    "        # fourcc = cv2.VideoWriter_fourcc(*'X264')  # Một lựa chọn thay thế\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (640, 640))  # Kích thước 640x640\n",
    "        if not out.isOpened():\n",
    "            print(\"Không thể khởi tạo VideoWriter. Kiểm tra codec H.264 có được hỗ trợ không.\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Fallback về mp4v nếu H264 không hoạt động\n",
    "            out = cv2.VideoWriter(output_path, fourcc, fps, (640, 640))\n",
    "            print(\"Đã chuyển sang codec mp4v.\")\n",
    "\n",
    "    # Data transform cho frame\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size=(640, 640)),\n",
    "    ])\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Chuyển frame từ BGR sang RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image_pil = Image.fromarray(frame_rgb)\n",
    "        image_tensor = data_transform(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "        # Dự đoán\n",
    "        with torch.no_grad():\n",
    "            predictions = model(image_tensor)[0]\n",
    "\n",
    "        # Lấy dự đoán\n",
    "        boxes = predictions[\"boxes\"]\n",
    "        labels = predictions[\"labels\"]\n",
    "        scores = predictions[\"scores\"]\n",
    "\n",
    "        # Áp dụng NMS\n",
    "        keep = nms(boxes, scores, iou_threshold=nms_threshold)\n",
    "        boxes = boxes[keep].cpu().numpy()\n",
    "        labels = labels[keep].cpu().numpy()\n",
    "        scores = scores[keep].cpu().numpy()\n",
    "\n",
    "        # Resize frame về 640x640\n",
    "        frame_640 = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # Vẽ bounding box\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            if score >= threshold:  # Ngưỡng score\n",
    "                x_min, y_min, x_max, y_max = map(int, box)\n",
    "                class_name = class_names[label - 1]\n",
    "                label_text = f\"{class_name}: {score:.2f}\"\n",
    "                cv2.rectangle(frame_640, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n",
    "                cv2.putText(frame_640, label_text, (x_min, y_min - 10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
    "\n",
    "        # Hiển thị frame\n",
    "        cv2.imshow('Object Detection', frame_640)\n",
    "\n",
    "        # Ghi frame vào video đầu ra nếu có\n",
    "        if output_path:\n",
    "            out.write(frame_640)\n",
    "\n",
    "        # Thoát nếu nhấn 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Đã dừng bởi người dùng. Đang lưu video...\")\n",
    "            break\n",
    "\n",
    "    # Giải phóng tài nguyên\n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()  # Đảm bảo video được lưu khi dừng\n",
    "        print(f\"Video đã được lưu tại: {output_path}\")\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def test_main():\n",
    "    video_path = 'data\\Traffic Camera VN(4).mp4'  # Đường dẫn tới file video\n",
    "    model_path = r\"model\\best_model (1).pt\"  # Đường dẫn tới model\n",
    "    class_names = [\"bus\", \"car\", \"motorbike\", \"truck\"]\n",
    "    nms_threshold = 0.8\n",
    "    output_path = 'data\\output\\output_video.mp4'  # Đường dẫn để lưu video đầu ra (tùy chọn)\n",
    "    threshold = 0.7\n",
    "\n",
    "    test_model(model_path, video_path, class_names, nms_threshold, output_path, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-03T03:25:12.749579Z",
     "iopub.status.busy": "2025-03-03T03:25:12.749307Z",
     "iopub.status.idle": "2025-03-03T03:25:15.163050Z",
     "shell.execute_reply": "2025-03-03T03:25:15.162075Z",
     "shell.execute_reply.started": "2025-03-03T03:25:12.749558Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã dừng bởi người dùng. Đang lưu video...\n",
      "Video đã được lưu tại: data\\output\\output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6480498,
     "sourceId": 10902493,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "pytorch_kernel",
   "language": "python",
   "name": "pytorch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
